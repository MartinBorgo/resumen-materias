La derivación numérica es un método utilizado en cálculo numérico para aproximar el valor de las derivadas de una función. Es especialmente útil cuando la función no tiene una forma analítica simple para su derivada o cuando se dispone de datos experimentales o de tablas de valores en lugar de una expresión algebraica de la función.

>[!tldr] Concepto Básico
>La derivada de una función $f(x)$ en un punto $x_0$ se define como el límite del cociente incremental cuando $h$ tiende a cero:
>$$
>f'(x_0) = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}
>$$
>
>En la derivación numérica, este límite se aproxima mediante una expresión algebraica con un valor pequeño pero finito de $h$.

### Análisis del Error

El error en la derivación numérica puede dividirse en dos componentes:

1. **Error de Truncamiento**: Proviene de la aproximación de la derivada utilizando diferencias finitas en lugar del límite exacto. Es función del tamaño del paso $h$ y del método utilizado.
2. **Error de Redondeo**: Surge debido a la precisión finita en las operaciones numéricas en la computadora. Este error se vuelve significativo cuando $h$ es muy pequeño.

## Métodos Comunes de Derivación Numérica

La derivación numérica se basa en aproximaciones de las derivadas utilizando valores discretos de la función en puntos específicos. A continuación se describen los métodos más comunes, junto con sus fórmulas matemáticas.

1. **Diferencias Finitas hacia Adelante**: Este método estima la derivada en un punto $x_0$ utilizando el valor de la función en $x_0$ y en un punto cercano hacia adelante $x_0 + h$:

$$
f'(x_0) \approx \frac{f(x_0 + h) - f(x_0)}{h}
$$


2. **Diferencias Finitas hacia Atrás**: Este método utiliza el valor de la función en $x_0$ y en un punto cercano hacia atrás $x_0 - h$ para estimar la derivada:

$$
f'(x_0) \approx \frac{f(x_0) - f(x_0 - h)}{h}
$$

>[!note] Error de Truncamiento
>El error en el método de diferencia finita hacia adelante y hacia atras son del orden $O(h)$, lo que indica que el error disminuye linealmente conforme se reduce $h$.

3. **Diferencias Finitas Centradas**: Este método toma un promedio de las diferencias hacia adelante y hacia atrás, utilizando los valores de la función en $x_0 + h$ y $x_0 - h$:

$$
f'(x_0) \approx \frac{f(x_0 + h) - f(x_0 - h)}{2h}
$$

>[!note] Error de Truncamiento en Diferencias Finitas Centradas
>El error en este método es del orden $O(h^2)$, lo que significa que es más preciso que las diferencias hacia adelante o hacia atrás. El error disminuye cuadráticamente al reducir $h$.

### Diferencias Finitas de Orden Superior

Para obtener una mejor aproximación de la derivada, se pueden utilizar diferencias finitas de orden superior. Estas fórmulas involucran más puntos alrededor de $x_0$. Por ejemplo, para aproximar la segunda derivada de una función $f(x)$ en un punto $x_0$, se utiliza la siguiente fórmula:

$$
f''(x_0) \approx \frac{f(x_0 + h) - 2f(x_0) + f(x_0 - h)}{h^2}
$$

Este método también se puede extender a derivadas de orden superior y diferencias hacia adelante y hacia atrás.
Las derivadas de orden superior también se pueden calcular usando diferencias finitas. Por ejemplo, la derivada cuarta de una función $f(x)$ en un punto $x_0$ puede aproximarse usando la siguiente fórmula:

$$
f^{(4)}(x_0) \approx \frac{f(x_0 - 2h) - 4f(x_0 - h) + 6f(x_0) - 4f(x_0 + h) + f(x_0 + 2h)}{h^4}
$$

Las fórmulas de diferencias finitas de orden superior se pueden generalizar para derivadas de cualquier orden $n$ utilizando combinaciones lineales de los valores de la función en varios puntos. Para una derivada de orden $k$ en el punto $x_0$, la fórmula de diferencias finitas de orden $p$ se puede escribir como:

$$
f^{(k)}(x_0) \approx \frac{1}{h^k} \sum_{i=-m}^m c_i f(x_0 + ih)
$$

Donde $c_i$ son los coeficientes determinados a partir de la expansión en serie de Taylor y $m$ depende del orden de la derivada y del nivel de precisión deseado.
### Extrapolación de Richardson

La ***extrapolación de Richardson*** es una técnica avanzada utilizada para mejorar la precisión de los métodos numéricos, especialmente en la derivación numérica y la integración numérica. La idea central detrás de esta técnica es combinar múltiples aproximaciones con diferentes valores de $h$ (el paso) para cancelar el error de truncamiento y obtener una mejor estimación de la derivada.
Supongamos que queremos aproximar la derivada de una función $f(x)$ en un punto $x_0$. Utilizando una fórmula de diferencias finitas, podemos calcular la derivada aproximada $D(h)$ para un cierto valor de $h$:

$$
D(h) = f'(x_0) + C h^p + O(h^{p+1})
$$

Donde $f'(x_0)$ es la derivada exacta que queremos estimar, $C$ es una constante, $p$ es el orden del método, y $O(h^{p+1})$ representa el término del error de truncamiento. Posteriormente, se utiliza otro paso $h/2$, se obtiene una segunda estimación:

$$
D\left(\frac{h}{2}\right) = f'(x_0) + C \left(\frac{h}{2}\right)^p + O\left(\left(\frac{h}{2}\right)^{p+1}\right)
$$

La extrapolación de Richardson combina estas dos estimaciones para eliminar el término de error $C h^p$, proporcionando una estimación de mayor precisión:

$$
R(h) = \frac{2^p D\left(\frac{h}{2}\right) - D(h)}{2^p - 1}
$$

Aquí, $R(h)$ es la nueva estimación mejorada de la derivada, donde el error de truncamiento ahora es del orden $O(h^{p+1})$, lo que significa que la precisión de la estimación ha aumentado.
En términos prácticos primero se calcula una aproximación inicial con nuestro valor de $h$ elegido, posteriormente se realiza una segunda estimación con paso $h/2$, una vez realizadas ambas aproximaciones, solo queda reemplazar los valores correspondientes en $R(h)$ y ya obtendremos la nueva aproximación.

>[!tldr] Ventajas y Desventajas de la Extrapolación de Richardson
>- **Ventajas**:
>	- **Mejora de la Precisión**: La técnica permite mejorar la precisión de las aproximaciones numéricas sin necesidad de reducir el paso $h$ a niveles muy pequeños, lo que podría introducir errores de redondeo.
>	- **Aplicabilidad General**: Aunque se utiliza comúnmente en derivación numérica, la extrapolación de Richardson también se puede aplicar a otros problemas numéricos, como la integración numérica y la solución de ecuaciones diferenciales.
>- **Desventajas**:
>	- **Costo Computacional**: La extrapolación de Richardson requiere cálculos adicionales, ya que implica evaluar la función en más puntos. Esto puede aumentar el costo computacional, especialmente si la función es costosa de evaluar.
>	- **Elección de $h$**: Aunque la técnica mejora la precisión, es crucial elegir un valor de $h$ adecuado para evitar que el error de redondeo domine cuando $h$ es muy pequeño.

### Elección del Método

Cuando decides qué método de derivación numérica aplicar, es importante considerar varios factores clave para asegurar que el método elegido sea el más adecuado para la situación específica. Aquí tienes algunas de las consideraciones fundamentales:

1. **Precisión Necesaria**: Si necesitas una alta precisión en tus cálculos, los métodos de orden superior, como las diferencias finitas centradas o la extrapolación de Richardson, suelen ser preferibles debido a su menor error de truncamiento.
2. **Disponibilidad de Datos**: Los métodos más avanzados, como las diferencias finitas de orden superior, requieren más puntos de datos alrededor del punto de interés. Si solo tienes datos en un rango limitado o solo en un lado del punto, podrías estar limitado a métodos más básicos como las diferencias hacia adelante o hacia atrás.
3. **Costo Computacional**: Los métodos de orden superior y la extrapolación de Richardson requieren más cálculos, lo que puede aumentar significativamente el tiempo de procesamiento.
4. **Error de Redondeo**: A medida que reduces el tamaño del paso $h$, el error de redondeo puede volverse significativo debido a las limitaciones de precisión en los cálculos numéricos. Es importante equilibrar el tamaño del paso $h$ para minimizar tanto el error de truncamiento como el error de redondeo.
5. **Naturaleza de la Función**: Ciertas funciones pueden tener comportamientos particulares, como discontinuidades o regiones donde la función cambia rápidamente.
6. **Orden de la Derivada**: El orden de la derivada que necesitas calcular también influye en la elección del método. Por ejemplo, si estás interesado en derivadas de segundo o tercer orden, necesitarás utilizar fórmulas de diferencias finitas de orden superior.
7. **Simetría de la Función**: En casos donde la función es simétrica o casi simétrica alrededor del punto de interés, las diferencias finitas centradas suelen ser más precisas y se prefieren sobre otros métodos.
8. **Combinación de Métodos**: En muchos casos, se pueden combinar métodos para obtener una aproximación medianamente buena a un coste bajo.
9. **Condiciones de Contorno**: Si trabajas con problemas donde las condiciones de contorno son críticas, como en ecuaciones diferenciales, puede que prefieras métodos que respeten mejor estas condiciones, como las diferencias finitas centradas o hacia atrás.

