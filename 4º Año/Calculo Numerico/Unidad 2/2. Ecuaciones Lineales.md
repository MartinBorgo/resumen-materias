Los sistemas de ecuaciones pueden ser de dos tipos, **incompatibles** los cuales no admiten solución y los **compatibles** los cuales admiten solución. Un sistema de ecuaciones se dice compatible y determinado si solo admite una única solución, mientras que un sistema de ecuaciones es compatible e indeterminado si admite más de una solución.

>[!tldr] Teorema de Rouchè-Frobenius
>Si el rango de $\rho(A)$ de la matriz de coeficientes $A$, es igual al rango de su matriz ampliada $\rho(A)$ entonces $A\cdot X=C$ es compatible.
>Un sistema compatible será determinado si el rango de la matriz de coeficientes es igual el número de incógnitas $\rho(A)=n$ y será indeterminado si el rango de la matriz de coeficientes es menor que el número de incógnitas $\rho(A)<n$.

La solución de un sistema de ecuaciones compatible de la forma $A\cdot X=C$ permanecen invariantes ante las siguientes situaciones:

1. Intercambio o permutación de filas o columnas.
2. Multiplicación de una fila o columna por un escalar $\lambda$ cualquiera distinto de cero.
3. Sumarle a una fila o columna otra línea multiplicada por un escalar $\lambda$ cualquiera.

## Solución de los Sistemas Lineales

Para la resolución de Sistemas de ecuaciones algebraicas lineales se utilizan dos tipos de métodos, los métodos directos y los métodos iterativos.
Los métodos directos son aquellos que obtiene la solución exacta, salvo errores de redondeo en los cálculos, luego de un número finito de operaciones elementales.

### Método de Reducción de Gauss

El método de eliminación de Gauss se explica mejor en un [video del profe Alex](https://www.youtube.com/watch?v=XRcx8-2lLJI) lo que hay en las filminas solo es confuso y solo es útil si se desea realizar una solución por computadora.
Una de las desventajas de este método es que durante el proceso en las fases de eliminación y sustitución es posible que ocurra una división entre cero. Por ello se ha desarrollado una estrategia del pivoteo que evita parcialmente estos problemas, y se recomienda usar doble precisión.
El pivoteo tiene dos finalidades: primero, superar la dificultas que presentan los coeficientes nulos de la diagonal, segundo, decrecer los errores de redondeo.

>[!tldr] Consideraciones a Tener en Cuenta
>Las técnicas para resolver sistemas lineales, necesitan considerar, además del espacio requerido para almacenamiento, el error de redondeo y el tiempo de procesamiento necesario para completar los cálculos, que dependen del número de operaciones aritméticas que se necesitan para resolver un problema rutinario.
>- El tiempo requerido para una multiplicación o división es aproximadamente el mismo, y es considerablemente mayor que el tiempo de una suma o resta.
>- La suma o resta insumen el mismo tiempo.
>- Las diferencias reales de tiempo de ejecución, dependen del sistema de cómputo usado.
### Método de Reducción de Gauss-Jordan

El método de eliminación de Gauss-Jordan también está explicado de mejor manera un [video del profe Alex](https://www.youtube.com/watch?v=dFmGzr1j6eY), lo que está en el material de clases es sobre todo para plantear un algoritmo.

### Factorización Triangular

Si el procedimiento de eliminación Gaussiana, puede aplicarse a un sistema $A\cdot X=C$, sin intercambio de renglones, entonces la matriz A puede factorizarse como el producto de
una matriz triangular inferior $L$ con una matriz triangular superior $U$. La forma de calcular estas matrices se explican con más claridad en [este video](https://www.youtube.com/watch?v=FBMhuvsDP_w). 
Esta descomposición de la matriz $A$ en las matrices $L$ y $U$, existe cuando se puede resolver de manera única el sistema $A \cdot X = C$, por eliminación gaussiana, sin intercambio de columnas o renglones.
El sistema $L \cdot U \cdot X = A \cdot X = C$, puede transformarse en el sistema $U\cdot X = L^{-1}\cdot C$, y como $U$ es triangular superior, se aplica sustitución hacia atrás. Las formas específicas de las matrices $L$ y $U$ se pueden obtener por eliminación gaussiana, pero lo deseable es hacer una sola sustitución hacia delante y otra hacia atrás.

