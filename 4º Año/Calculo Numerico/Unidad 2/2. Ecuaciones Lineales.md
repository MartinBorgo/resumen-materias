Los sistemas de ecuaciones pueden ser de dos tipos, **incompatibles** los cuales no admiten solución y los **compatibles** los cuales admiten solución. Un sistema de ecuaciones se dice compatible y determinado si solo admite una única solución, mientras que un sistema de ecuaciones es compatible e indeterminado si admite más de una solución.
>[!tldr] Teorema de Rouchè-Frobenius
>Si el rango de $\rho(A)$ de la matriz de coeficientes $A$, es igual al rango de su matriz ampliada $\rho(A)$ entonces $A\cdot X=C$ es compatible.
>Un sistema compatible será determinado si el rango de la matriz de coeficientes es igual el número de incógnitas $\rho(A)=n$ y será indeterminado si el rango de la matriz de coeficientes es menor que el número de incógnitas $\rho(A)<n$.

La solución de un sistema de ecuaciones compatible de la forma $A\cdot X=C$ permanecen invariantes ante las siguientes situaciones:
1. Intercambio o permutación de filas o columnas.
2. Multiplicación de una fila o columna por un escalar $\lambda$ cualquiera distinto de cero.
3. Sumarle a una fila o columna otra línea multiplicada por un escalar $\lambda$ cualquiera.

## Solución de los Sistemas Lineales
Para la resolución de Sistemas de ecuaciones algebraicas lineales se utilizan dos tipos de métodos, los métodos directos y los métodos iterativos.
Los métodos directos son aquellos que obtiene la solución exacta, salvo errores de redondeo en los cálculos, luego de un número finito de operaciones elementales.
## Métodos Directos
Estos métodos se caracterizan por tener una cantidad exacta de pasos, las cuales va a variar de acuerdo al tamaño de la matriz que se esté resolviendo.
#### Método de Reducción de Gauss
El método de eliminación de Gauss se explica mejor en un [video del profe Alex](https://www.youtube.com/watch?v=XRcx8-2lLJI) lo que hay en las filminas solo es confuso y solo es útil si se desea realizar una solución por computadora.
Una de las desventajas de este método es que durante el proceso en las fases de eliminación y sustitución es posible que ocurra una división entre cero. Por ello se ha desarrollado una estrategia del pivoteo que evita parcialmente estos problemas, y se recomienda usar doble precisión.
El pivoteo tiene dos finalidades: primero, superar la dificultad que presentan los coeficientes nulos de la diagonal, segundo, decrecer los errores de redondeo.
>[!tldr] Consideraciones a Tener en Cuenta
>Las técnicas para resolver sistemas lineales, necesitan considerar, además del espacio requerido para almacenamiento, el error de redondeo y el tiempo de procesamiento necesario para completar los cálculos, que dependen del número de operaciones aritméticas que se necesitan para resolver un problema rutinario.
>- El tiempo requerido para una multiplicación o división es aproximadamente el mismo, y es considerablemente mayor que el tiempo de una suma o resta.
>- La suma o resta insumen el mismo tiempo.
>- Las diferencias reales de tiempo de ejecución, dependen del sistema de cómputo usado.
#### Método de Reducción de Gauss-Jordan
El método de eliminación de Gauss-Jordan también está explicado de mejor manera un [video del profe Alex](https://www.youtube.com/watch?v=dFmGzr1j6eY), lo que está en el material de clases es sobre todo para plantear un algoritmo.
En general, a nivel computacional, este método no se utiliza mucho, ya que requiere mucho más cálculos, lo que resulta en más errores de redondeo.
#### Factorización Triangular
Si el procedimiento de eliminación Gaussiana, puede aplicarse a un sistema $A\cdot X=C$, sin intercambio de renglones, entonces la matriz $A$ puede factorizarse como el producto de una matriz triangular inferior $L$ con una matriz triangular superior $U$. La forma de calcular estas matrices se explican con más claridad en [este video](https://www.youtube.com/watch?v=FBMhuvsDP_w). 
Esta descomposición de la matriz $A$ en las matrices $L$ y $U$, existe cuando se puede resolver de manera única el sistema $A \cdot X = C$, por eliminación gaussiana, sin intercambio de columnas o renglones.
El sistema $L \cdot U \cdot X = A \cdot X = C$, puede transformarse en el sistema $U\cdot X = L^{-1}\cdot C$, y como $U$ es triangular superior, se aplica sustitución hacia atrás. Las formas específicas de las matrices $L$ y $U$ se pueden obtener por eliminación gaussiana, pero lo deseable es hacer una sola sustitución hacia delante y otra hacia atrás.
## Métodos Iterativos
Al contrario que los métodos directos que nos dan un resultado concreto luego de realizar una determinada cantidad de pasos, los métodos iterativos lo que hacen en buscar una aproximación aplicando un número desconocido de pasos.
>[!example] Norma de Vectores y Matrices
>Al realizar aproximaciones para calcular el valor de los coeficientes de una matriz se debe saber si dicho error es grande, en un sistema de ecuaciones lineales de la forma $X = A^{-1} \cdot C$, el error es grande si la matriz inversa lo es, y es pequeña si dicha matriz también lo es, cabe aclarar que el tamaño de una matriz no está relacionado con su orden, sino que se utiliza las normas de las matrices para denotar su tamaño.
>Estas normas pueden ser columna-suma o fila-suma denotada como $||A||_1$, en la cual se calcula la suma de los calores absolutos de las columnas o filas y el mayor de esos resultados representaría el tamaño de matriz, otra norma utilizada es la norma de magnitud máxima que toma como tamaño de la matriz el mayor número de la matriz en su valor absoluto. La norma de Fröbenius es otra medida del tamaño de las matrices y se consigue al sumar todos los elementos de la matriz al cuadrado y luego al resultado aplicarle una raíz cuadrada.
>
> #### Número de Condición de una Matriz
>El número de condición de una matriz es una medida que proporciona información sobre la sensibilidad de la solución de un sistema de ecuaciones lineales a cambios en los términos independientes o en los elementos de la matriz. En términos más específicos, este número ayuda a entender cómo el error en los datos de entrada se amplifica en el resultado debido a las operaciones numéricas realizadas durante la solución del sistema. Este término se calcula de la siguiente forma:
>$$
>k(A) = ||A|| \cdot ||A^{-1}||
>$$
>Mientras más cercano a 1 sea este término mejor condicionada estará la matriz. Esta medida se puede utilizar para medir el error relativo, de la siguiente forma:
>$$
>\frac{||\Delta X||}{||X||} \leq k(A) \cdot \frac{||\Delta A||}{||A||}
>$$
#### Método de Jacobi
Si se considera un sistema de ecuaciones algebraicas, que puede escribirse en forma matricial como $A \cdot X = C$ y que $A = D + R$, donde $D$ es una matriz diagonal. Entonces puede escribirse que:
$$
(D + R) \cdot X = C \rightarrow DX = C - RX \rightarrow X = D^{-1} \cdot C - D^{-1} \cdot RX
$$
Para aplicar este método, se considera una primera aproximación al valor de las incógnitas $x$, que se denomina $X^{(0)}$. Se sustituye esa primera aproximación en los demás miembros de la ecuación y así sucesivamente, por ejemplo, en una situación trivial, dado una matriz y teniendo lista la ecuación, todos los $x_i$ pueden ser reemplazados por 0 para obtener la primera aproximación de las incógnitas. El procedimiento se repite hasta que la solución converja cerca de los valores reales, con la diferencia que para la aproximación de primer grado $X^{(1)}$, se utilizaran las anteriores aproximaciones. La convergencia se puede verificar usando el criterio de error relativo.
>[!caution] Utilización del Método de Jacobi
>Este método es muy poco utilizado debido a que el método de Gauss-Seidel converge más rápidamente a la solución y además lo hace cuando no se logra que el método de Jacobi converja.
>La condición suficiente para que el método de Jacobi converja es que la matriz de coeficientes sea diagonal dominante, es decir que cada elemento de la diagonal principal es mayor en valor absoluto que la suma del resto de los elementos de la misma fila en la que se encuentra el elemento en cuestión.
#### Método de Gauss-Seidel
Es un método iterativo que disminuye el error de redondeo, se denomina también de desplazamientos sucesivos o de iteraciones parciales. Sí, se tiene un conjunto de $n$ ecuaciones. La convergencia se puede verificar usando el criterio de error relativo. Este método se diferencia del de Jacobi, puesto que una vez que se calcula una aproximación a una incógnita se utiliza esta aproximación en la misma iteración para calcular las siguientes incógnitas.
Las condiciones suficientes para que el método de Gauss-Seidel converja es que la matriz de coeficientes sea diagonal dominante o bien que la matriz de coeficientes sea simétrica y definida positiva.
>[!tldr] Criterio de Error Relativo
>Para saber cuando dejar de iterar, por lo general se emplea el criterio de error relativo. En pocas palabras se calcula el error relativo de las aproximaciones de cada una de las aproximaciones obtenidas y se comprueba que estos errores sean menores a un error máximo soportado, el cual es fijado de forma arbitraria. La fórmula del cálculo es la siguiente:
>$$
>|\frac{x_n^{(k)} - x_n^{(k-1)}}{x_n^{(k)}}|\cdot 100
>$$
>En donde $k$ indica el número de aproximaciones, para la primera aproximación $X^{(0)}$, no se calcula el error relativo, se hace desde la segunda aproximación en adelante.

>[!caution] Condición de Convergencia
>Al igual que el método de Jacobi, el método de Gauss-Seidel requieren que la matriz sea diagonal dominante, ya que puede que el método no converja adecuadamente o directamente no converja.
## Mejora de las Soluciones
Por lo general, el remedio más simple para el mal condicionamiento es emplear más cifras significativas en los cálculos. Eso es posible, si la computadora lo permite, reducirá el problema, pero habrá que gastar más recursos y tiempo, en memoria y en cálculos.
En el caso del pivoteo, se debe evitar que en la diagonal principal aparezca algún 0, para lograr eso antes de normalizar cada renglón, resulta conveniente determinar coeficiente más grande disponible en la columna debajo del elemento pivote. Los renglones se pueden intercambiar, de forma que el elemento más grande sea el elemento pivote; esto se conoce como ***pivoteo parcial***.

