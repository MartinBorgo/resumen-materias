Las ecuaciones diferenciales son ecuaciones en las que la incógnita es una función y aparecen una o más de sus derivadas con respecto a una o más variables independientes. Estas ecuaciones modelan fenómenos donde intervienen cambios, como los fenómenos naturales o leyes del universo.
Una ecuación diferencial ordinaria (EDO) relaciona una función $y(x)$ con sus derivadas de la forma: $F(x, y, y', y'', \dots, y^{(n)}) = 0$ donde $y$ es la función incógnita, $x$ es la variable independiente, y $y', y'', y^{(n)}$ son las derivadas de $y$. Las ecuaciones diferenciales se clasifican en:
- **Ecuaciones diferenciales ordinarias (EDO):** Estas involucran derivadas respecto a una única variable independiente, como $x$. Ejemplos típicos incluyen las ecuaciones de primer o segundo orden.
- **Ecuaciones diferenciales en derivadas parciales (EDP):** Involucran derivadas parciales respecto a dos o más variables independientes.

El **orden** de una ecuación diferencial es el de la derivada de mayor orden que aparece en la ecuación. Por ejemplo, $y' = f(x, y)$ es de primer orden, mientras que $y'' = f(x, y, y')$ es de segundo orden. El **grado** de una ecuación diferencial está dado por el exponente de la derivada de mayor orden. En el ejemplo anterior, todas las ecuaciones son de primer grado.
### Soluciones de Ecuaciones Diferenciales
La solución de una ecuación diferencial es la función $y = f(x)$ que satisface la ecuación en un intervalo $I$. Esto significa que al sustituir $y$ y sus derivadas en la ecuación diferencial, esta debe cumplirse para todos los valores de $x$ en $I$.
En general, las soluciones de ecuaciones diferenciales incluyen una **constante de integración**, para determinar el valor de esta constante, es necesario imponer **condiciones iniciales**. Por ejemplo, dada la ecuación diferencial $y' = f(x, y)$ con la condición inicial $y(x_0) = y_0$, se busca una función $y = f(x)$ que cumpla esta ecuación en un intervalo dado.
>[!tldr] Teorema de Cauchy
>El **Teorema de Cauchy** establece las condiciones bajo las cuales una ecuación diferencial tiene una solución única. El teorema dice lo siguiente:
>- Si la función $f(x, y)$ es **analítica** (es decir, tiene derivadas continuas de todos los órdenes) en un dominio que contiene el punto $(x_0, y_0)$, entonces existe una única solución para el problema de valor inicial dado.
>
>Este teorema es clave en la teoría de ecuaciones diferenciales, ya que garantiza la **existencia** y **unicidad** de la solución, pero solo bajo condiciones estrictas de analiticidad de $f(x, y)$. El **Teorema de Cauchy** es restrictivo, porque exige que la función sea analítica. Sin embargo, una condición menos exigente que también asegura la existencia y unicidad de la solución es la **Condición de Lipschitz**, la cual dice lo siguiente:
>
>- Una función $f(x, y)$ satisface la **Condición de Lipschitz** respecto a la variable $y$ en un conjunto $D \subset \mathbb{R}^2$ si existe una constante $L > 0$ tal que  $|f(x, y_1) - f(x, y_2)| \leq L |y_1 - y_2|$ para todo $(x, y_1), (x, y_2) \in D$. La constante $L$ se llama **constante de Lipschitz** para $f$.
>
>Esta condición tiene otro teorema asociado que es el que nos interesa ya que nos permite resolver ecuaciones diferenciales de forma numérica. El teorema dice lo siguiente:
>
>- Si $f(x, y)$ está definida en un conjunto convexo $D \subset \mathbb{R}^2$ y satisface la condición de Lipschitz con constante $L$, entonces existe una solución única para el problema de valor inicial en $D$.
> 
>Esta condición de Lipschitz es suficiente, pero no siempre necesaria para garantizar la existencia de solución única. Por ejemplo, una función simple que cumple con la condición de Lipschitz es $f(x, y) = y^2$, donde $\frac{\partial f}{\partial y} = 2y$ está acotada en un dominio dado.
### Solución Numérica de Ecuaciones Diferenciales Ordinarias
Los **métodos numéricos** para resolver ecuaciones diferenciales ordinarias con condiciones iniciales buscan aproximar la solución en puntos discretos. A continuación se presentan los métodos más comunes:
#### Método de Euler
Este es el método más básico para resolver EDO. Se basa en aproximar la derivada en un punto para avanzar a lo largo del eje de la variable independiente (generalmente el tiempo).
$$y_{n+1} = y_n + h \cdot f(t_n, y_n)$$
Donde $h$ es el tamaño de paso, y $f(t_n, y_n)$ es la función que define la derivada de $y$ en $t_n$. Este método es **explícito**, ya que el nuevo valor $y_{n+1}$ se calcula directamente a partir de $y_n$.
#### Método de Euler Modificado
El **Método de Euler Modificado** mejora la precisión al utilizar una aproximación en el punto medio del intervalo, en lugar de solo en el extremo inicial. Este método consta de dos pasos:
1. Calcular una primera aproximación de $y_{n+1}$ usando el método de Euler:
$$y_{n+1}^{(0)} = y_n + h \cdot f(t_n, y_n)$$
2. El cálculo de la aproximación se utiliza en la fórmula modificada utiliza para refinar el cálculo:
$$y_{n+1} = y_n + \frac{h}{2} \left(f(t_n, y_n) + f(t_{n+1}, y_{n+1}^{(0)})\right)$$
Este método es de **segundo orden** de precisión y reduce significativamente el error local.
#### Método de Runge-Kutta de Segundo Orden
El método de **Runge-Kutta** mejora aún más la precisión utilizando un promedio ponderado de varias evaluaciones de la función. La fórmula es la siguiente:
$$y_{n+1} = y_n + h \cdot \phi(t_n, y_n, h)$$
Donde $\phi$ es la función de incremento, calculada como un promedio de la pendiente al inicio y al final del intervalo:
$$\phi(t_n, y_n, h) = \frac{1}{2} \left( f(t_n, y_n) + f(t_n + h, y_n + h \cdot f(t_n, y_n)) \right)$$
Este método también es conocido como el **método del punto medio**.
#### Método de Runge-Kutta de Cuarto Orden
El **método de Runge-Kutta de cuarto orden (RK4)** es uno de los métodos más precisos y populares, ya que logra un error local de orden $O(h^5)$ y un error global de $O(h^4)$.
$$y_{n+1} = y_n + \frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4)$$
Donde los distintos $k_i$ se calculan de la siguiente manera:
- $k_1 = f(t_n, y_n)$
- $k_2 = f(t_n + \frac{h}{2}, y_n + \frac{h}{2} k_1)$
- $k_3 = f(t_n + \frac{h}{2}, y_n + \frac{h}{2} k_2)$
- $k_4 = f(t_n + h, y_n + h k_3)$

Este método utiliza cuatro evaluaciones de la función en cada paso, logrando una alta precisión incluso con un tamaño de paso relativamente grande.
#### Métodos Predictor-Corrector
Los **métodos multipasos** utilizan la información de varios puntos anteriores, en lugar de solo el último punto, para calcular la solución en el siguiente paso. Este método como tal utiliza dos métodos en principio independientes, pero que en conjunto dan origen a este método, los cuales son el método de ***Adams-Bashforth*** y el método de ***Adams-Moulton***
El método de Adams-Bashforth es un método explícito que utiliza valores previos para calcular la solución en el siguiente paso. Dependiendo la cantidad de puntos que usemos para calcular el nuevo paso se definirá el grado del método:
- Segundo grado.
$$y_{n+1} = y_n + \frac{h}{2} (3f(t_n, y_n) - f(t_{n-1}, y_{n-1}))$$
- Tercer grado.
$$y_{n+1} = y_n + \frac{h}{12} (23f(t_n, y_n) - 16f(t_{n-1}, y_{n-1}) + 5f(t_{n-2}, y_{n-2}))$$
- Cuarto grado.
$$y_{n+1} = y_n + \frac{h}{24} (55f(t_n, y_n) - 59f(t_{n-1}, y_{n-1}) + 37f(t_{n-2}, y_{n-2}) - 9f(t_{n-3}, y_{n-3}))$$

En cada ecuación $y_n$ es la última aproximación calculada y $t_n$ el paso para dicha aproximación.
El método de Adams-Moulton es implícito, lo que significa que requiere resolver una ecuación en cada paso. Y al igual que el método de Adams-Bashforth el grado del método dependerá de la cantidad de puntos que se utilice para calcular la aproximación.
- Segundo grado.
$$y_{n+1} = y_n + \frac{h}{2} (f(t_{n+1}, y_{n+1}) + f(t_n, y_n))$$
- Tercer grado.
$$y_{n+1} = y_n + \frac{h}{12} (5f(t_{n+1}, y_{n+1}) + 8f(t_n, y_n) - f(t_{n-1}, y_{n-1}))$$
- Cuarto grado.
$$y_{n+1} = y_n + \frac{h}{24} (9f(t_{n+1}, y_{n+1}) + 19f(t_n, y_n) - 5f(t_{n-1}, y_{n-1}) + f(t_{n-2}, y_{n-2}))$$
**Fórmula de un paso:**
$$y_{n+1} = y_n + \frac{h}{2} \left( f(t_{n+1}, y_{n+1}) + f(t_n, y_n) \right)$$
Al ser implícito, se necesita calcular iterativamente $y_{n+1}$, lo que aumenta la precisión y la estabilidad.
El método predictor corrector en sí consiste en utilizar primero el método de Adams-Bashforth para calcular una nueva aproximación mucho más exacta y luego utilizar esta misma aproximación en el método de Adams-Multon.
>[!tldr] Errores en los Métodos Numéricos
>- **Error de truncamiento local**: Es el error que se comete en cada paso debido a la aproximación de la solución en ese punto. El error local se reduce con un tamaño de paso menor.
>- **Error global**: Es el error acumulado en todos los pasos hasta el punto final. Un método con bajo error local no necesariamente tiene bajo error global.
> 
>Cada método tiene un **orden de convergencia** que indica cuán rápido disminuye el error cuando se reduce el tamaño de paso $h$. Los métodos de mayor orden son más precisos, pero también requieren más cálculos.

### Solución Numérica para Sistemas de Ecuaciones Diferenciales Ordinarias
Un **sistema de ecuaciones diferenciales ordinarias** (EDO) involucra múltiples ecuaciones interrelacionadas. Las variables dependientes en estos sistemas dependen de una o más variables independientes. Los métodos para resolver sistemas de ecuaciones diferenciales son una extensión de los métodos utilizados para ecuaciones individuales.
#### Método de Euler para Sistemas
El **método de Euler** para sistemas de EDO es una extensión del método de Euler para una ecuación. Se usa la misma idea de avanzar en pequeños pasos, pero ahora se aplica a cada ecuación en el sistema.
$$\mathbf{y}_{n+1} = \mathbf{y}_n + h \cdot \mathbf{f}(t_n, \mathbf{y}_n)$$
Donde $\mathbf{y}_n$ es el vector de las incógnitas $y_1, y_2, \dots, y_m$ en el paso $n$, y $\mathbf{f}$ es el vector de las funciones derivadas correspondientes.
#### Método de Runge-Kutta para Sistemas
El método de **Runge-Kutta de cuarto orden** también se puede aplicar a sistemas de ecuaciones diferenciales. Se calcula cada componente del vector $\mathbf{y}$ utilizando las mismas fórmulas que para el caso de una ecuación única.
$$\mathbf{y}_{n+1} = \mathbf{y}_n + \frac{h}{6} \left( \mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 + \mathbf{k}_4 \right)$$
Donde los distintos $\mathbf{k}_i$ se calculan de la siguiente manera:
- $\mathbf{k}_1 = \mathbf{f}(t_n, \mathbf{y}_n)$
- $\mathbf{k}_2 = \mathbf{f}(t_n + \frac{h}{2}, \mathbf{y}_n + \frac{h}{2} \mathbf{k}_1)$
- $\mathbf{k}_3 = \mathbf{f}(t_n + \frac{h}{2}, \mathbf{y}_n + \frac{h}{2} \mathbf{k}_2)$
- $\mathbf{k}_4 = \mathbf{f}(t_n + h, \mathbf{y}_n + h \mathbf{k}_3)$

Este método es adecuado para resolver sistemas de ecuaciones diferenciales simultáneamente.
#### Método Predictor-corrector para Sistemas
El método **Adams-Bashforth**, al igual que los anteriores se modifica levemente para poder solucionar sistemas de ecuaciones, quedándonos de la siguiente manera.
$$\mathbf{y}_{n+1} = \mathbf{y}_n + \frac{h}{2} \left( 3\mathbf{f}(t_n, \mathbf{y}_n) - \mathbf{f}(t_{n-1}, \mathbf{y}_{n-1}) \right)$$
El método **Adams-Moulton**, también se puede aplicar a sistemas de ecuaciones diferenciales. En este caso, se resuelve el sistema de ecuaciones simultáneamente en cada paso. La fórmula correspondiente es la siguiente:
$$\mathbf{y}_{n+1} = \mathbf{y}_n + \frac{h}{2} \left( \mathbf{f}(t_{n+1}, \mathbf{y}_{n+1}) + \mathbf{f}(t_n, \mathbf{y}_n) \right)$$
Como se puede apreciar todos los métodos expuestos anteriormente solo se modifican levemente y te permiten calcular aproximaciones de forma adecuada. Para el caso del método predictor-corrector solo se mostraron los métodos de Adams-Bashforth y Adams-Multon de grado o paso dos, ya que la fórmula en sí es la misma solo que se aplica sobre diferentes ecuaciones.