### **Definición de ecuaciones diferenciales**
Las ecuaciones diferenciales son ecuaciones en las que la incógnita es una función y aparecen una o más de sus derivadas con respecto a una o más variables independientes. Estas ecuaciones modelan fenómenos donde intervienen cambios, como los fenómenos naturales o leyes del universo.

**Ejemplo básico:**
- Una ecuación diferencial ordinaria (EDO) relaciona una función $y(x)$ con sus derivadas de la forma: $F(x, y, y', y'', \dots, y^{(n)}) = 0$ donde $y$ es la función incógnita, $x$ es la variable independiente, y $y', y'', y^{(n)}$ son las derivadas de $y$.

### **Clasificación de las ecuaciones diferenciales**
Las ecuaciones diferenciales se clasifican en:
- **Ecuaciones diferenciales ordinarias (EDO):** Estas involucran derivadas respecto a una única variable independiente, como $x$. Ejemplos típicos incluyen las ecuaciones de primer o segundo orden.
- **Ecuaciones diferenciales en derivadas parciales (EDP):** Involucran derivadas parciales respecto a dos o más variables independientes.

#### Orden y Grado:
- El **orden** de una ecuación diferencial es el de la derivada de mayor orden que aparece en la ecuación. Por ejemplo:
  - $y' = f(x, y)$ es de primer orden.
  - $y'' = f(x, y, y')$ es de segundo orden.
- El **grado** de una ecuación diferencial está dado por el exponente de la derivada de mayor orden. En el ejemplo anterior, todas las ecuaciones son de primer grado.

### **Soluciones de ecuaciones diferenciales**
La **solución** de una ecuación diferencial es la función $y = f(x)$ que satisface la ecuación en un intervalo $I$. Esto significa que al sustituir $y$ y sus derivadas en la ecuación diferencial, ésta debe cumplirse para todos los valores de $x$ en $I$.

En general, las soluciones de ecuaciones diferenciales incluyen una **constante de integración**. Para determinar el valor de esta constante, es necesario imponer **condiciones iniciales**.

#### Ejemplo de problema con valor inicial:
Dada la ecuación diferencial:
$y' = f(x, y)$
con la condición inicial $y(x_0) = y_0$, se busca una función $y = f(x)$ que cumpla esta ecuación en un intervalo dado.

### **Teorema de existencia y unicidad**
El **Teorema de Cauchy** establece las condiciones bajo las cuales una ecuación diferencial tiene una solución única:
- Si la función $f(x, y)$ es **analítica** (es decir, tiene derivadas continuas de todos los órdenes) en un dominio que contiene el punto $(x_0, y_0)$, entonces existe una única solución para el problema de valor inicial dado.

Este teorema es clave en la teoría de ecuaciones diferenciales, ya que garantiza la **existencia** y **unicidad** de la solución, pero solo bajo condiciones estrictas de analiticidad de $f(x, y)$.

### **Condición de Lipschitz**
El **Teorema de Cauchy** es restrictivo, ya que exige que la función sea analítica. Sin embargo, una condición menos exigente que también asegura la existencia y unicidad de la solución es la **Condición de Lipschitz**.

#### Definición:
Una función $f(x, y)$ satisface la **Condición de Lipschitz** respecto a la variable $y$ en un conjunto $D \subset \mathbb{R}^2$ si existe una constante $L > 0$ tal que:
$|f(x, y_1) - f(x, y_2)| \leq L |y_1 - y_2|$
para todo $(x, y_1), (x, y_2) \in D$. La constante $L$ se llama **constante de Lipschitz** para $f$.

#### Teorema asociado:
Si $f(x, y)$ está definida en un conjunto convexo $D \subset \mathbb{R}^2$ y satisface la condición de Lipschitz con constante $L$, entonces existe una solución única para el problema de valor inicial en $D$.

- Esta condición de Lipschitz es suficiente, pero no siempre necesaria para garantizar la existencia de solución única.
  
#### Ejemplo:
Una función simple que cumple con la condición de Lipschitz es $f(x, y) = y^2$, donde $\frac{\partial f}{\partial y} = 2y$ está acotada en un dominio dado.

### **Solución numérica de ecuaciones diferenciales ordinarias (EDO) con condiciones iniciales: métodos de un paso**

Los **métodos numéricos** para resolver ecuaciones diferenciales ordinarias con condiciones iniciales buscan aproximar la solución en puntos discretos. A continuación se presentan los métodos más comunes:

#### **Método de Euler**
Este es el método más básico para resolver EDO. Se basa en aproximar la derivada en un punto para avanzar a lo largo del eje de la variable independiente (generalmente el tiempo).

**Fórmula:**
$y_{n+1} = y_n + h \cdot f(t_n, y_n)$
- Donde $h$ es el tamaño de paso, y $f(t_n, y_n)$ es la función que define la derivada de $y$ en $t_n$.

Este método es **explícito**, ya que el nuevo valor $y_{n+1}$ se calcula directamente a partir de $y_n$.

#### **Método de Euler Modificado**
El **Método de Euler Modificado** mejora la precisión al utilizar una aproximación en el punto medio del intervalo, en lugar de solo en el extremo inicial.

**Pasos:**
1. Calcular una primera aproximación de $y_{n+1}$ usando el método de Euler:
   $y_{n+1}^{(0)} = y_n + h \cdot f(t_n, y_n)$
2. Luego, la fórmula modificada utiliza esta primera aproximación para refinar el cálculo:
   $y_{n+1} = y_n + \frac{h}{2} \left(f(t_n, y_n) + f(t_{n+1}, y_{n+1}^{(0)})\right)$

Este método es de **segundo orden** de precisión y reduce significativamente el error local.

#### **Método de Runge-Kutta de Segundo Orden**
El método de **Runge-Kutta** mejora aún más la precisión utilizando un promedio ponderado de varias evaluaciones de la función.

**Fórmula:**
$y_{n+1} = y_n + h \cdot \phi(t_n, y_n, h)$

Donde $\phi$ es la función de incremento, calculada como un promedio de la pendiente al inicio y al final del intervalo:

$\phi(t_n, y_n, h) = \frac{1}{2} \left( f(t_n, y_n) + f(t_n + h, y_n + h \cdot f(t_n, y_n)) \right)$

Este método también es conocido como el **método del punto medio**.

#### **Método de Runge-Kutta de Cuarto Orden**
El **método de Runge-Kutta de cuarto orden (RK4)** es uno de los métodos más precisos y populares, ya que logra un error local de orden $O(h^5)$ y un error global de $O(h^4)$.

**Fórmula:**
$y_{n+1} = y_n + \frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4)$
Donde:
- $k_1 = f(t_n, y_n)$
- $k_2 = f(t_n + \frac{h}{2}, y_n + \frac{h}{2} k_1)$
- $k_3 = f(t_n + \frac{h}{2}, y_n + \frac{h}{2} k_2)$
- $k_4 = f(t_n + h, y_n + h k_3)$

Este método utiliza cuatro evaluaciones de la función en cada paso, logrando una alta precisión incluso con un tamaño de paso relativamente grande.

#### **Métodos Multipasos: Predictor-Corrector**

Los **métodos multipasos** utilizan la información de varios puntos anteriores, en lugar de solo el último punto, para calcular la solución en el siguiente paso.

#### **Método de Adams-Bashforth**
El **método de Adams-Bashforth** es un método **explícito** que utiliza valores previos para calcular la solución en el siguiente paso.

**Fórmula de dos pasos:**

$y_{n+1} = y_n + \frac{h}{2} \left( 3 f(t_n, y_n) - f(t_{n-1}, y_{n-1}) \right)$

Este método tiene versiones de más pasos (por ejemplo, de tres y cuatro pasos), que utilizan más valores previos de $f(t, y)$.

#### **Método de Adams-Moulton**
El **método de Adams-Moulton** es

 **implícito**, lo que significa que requiere resolver una ecuación en cada paso.

**Fórmula de un paso:**

$y_{n+1} = y_n + \frac{h}{2} \left( f(t_{n+1}, y_{n+1}) + f(t_n, y_n) \right)$

Al ser implícito, se necesita calcular iterativamente $y_{n+1}$, lo que aumenta la precisión y la estabilidad.

#### **Método Predictor-Corrector de Adams-Bashforth-Moulton**
Este método combina el predictor **explícito** de Adams-Bashforth con el corrector **implícito** de Adams-Moulton para mejorar la precisión y estabilidad.

**Fórmula del predictor (Adams-Bashforth de cuatro pasos):**

$y_{n+1}^{*} = y_n + \frac{h}{24} \left( 55 f(t_n, y_n) - 59 f(t_{n-1}, y_{n-1}) + 37 f(t_{n-2}, y_{n-2}) - 9 f(t_{n-3}, y_{n-3}) \right)$

**Fórmula del corrector (Adams-Moulton de cuatro pasos):**

$y_{n+1} = y_n + \frac{h}{24} \left( 9 f(t_{n+1}, y_{n+1}^{*}) + 19 f(t_n, y_n) - 5 f(t_{n-1}, y_{n-1}) + f(t_{n-2}, y_{n-2}) \right)$

Este método es muy utilizado en la práctica porque combina lo mejor de los métodos explícitos e implícitos.

#### **Errores en los métodos numéricos**

- **Error de truncamiento local**: Es el error que se comete en cada paso debido a la aproximación de la solución en ese punto. El error local se reduce con un tamaño de paso menor.
- **Error global**: Es el error acumulado en todos los pasos hasta el punto final. Un método con bajo error local no necesariamente tiene bajo error global.

Cada método tiene un **orden de convergencia** que indica cuán rápido disminuye el error cuando se reduce el tamaño de paso $h$. Los métodos de mayor orden son más precisos, pero también requieren más cálculos.

### **Métodos de solución para sistemas de ecuaciones diferenciales ordinarias**

Un **sistema de ecuaciones diferenciales ordinarias** (EDO) involucra múltiples ecuaciones interrelacionadas. Las variables dependientes en estos sistemas dependen de una o más variables independientes. Los métodos para resolver sistemas de ecuaciones diferenciales son una extensión de los métodos utilizados para ecuaciones individuales.

#### **Método de Euler para sistemas**
El **método de Euler** para sistemas de EDO es una extensión del método de Euler para una ecuación. Se usa la misma idea de avanzar en pequeños pasos, pero ahora se aplica a cada ecuación en el sistema.

**Fórmula:**
$\mathbf{y}_{n+1} = \mathbf{y}_n + h \cdot \mathbf{f}(t_n, \mathbf{y}_n)$

Donde $\mathbf{y}_n$ es el vector de las incógnitas $y_1, y_2, \dots, y_m$ en el paso $n$, y $\mathbf{f}$ es el vector de las funciones derivadas correspondientes.

#### **Método de Runge-Kutta para sistemas**
El método de **Runge-Kutta de cuarto orden** también se puede aplicar a sistemas de ecuaciones diferenciales. Se calcula cada componente del vector $\mathbf{y}$ utilizando las mismas fórmulas que para el caso de una ecuación única.

**Fórmula:**
$\mathbf{y}_{n+1} = \mathbf{y}_n + \frac{h}{6} \left( \mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 + \mathbf{k}_4 \right)$
Donde:
- $\mathbf{k}_1 = \mathbf{f}(t_n, \mathbf{y}_n)$
- $\mathbf{k}_2 = \mathbf{f}(t_n + \frac{h}{2}, \mathbf{y}_n + \frac{h}{2} \mathbf{k}_1)$
- $\mathbf{k}_3 = \mathbf{f}(t_n + \frac{h}{2}, \mathbf{y}_n + \frac{h}{2} \mathbf{k}_2)$
- $\mathbf{k}_4 = \mathbf{f}(t_n + h, \mathbf{y}_n + h \mathbf{k}_3)$

Este método es adecuado para resolver sistemas de ecuaciones diferenciales simultáneamente.

#### **Método de Adams-Bashforth para sistemas**
El método **Adams-Bashforth**, un método multipasos explícito, también se puede aplicar a sistemas de EDO.

**Fórmula de dos pasos:**
$\mathbf{y}_{n+1} = \mathbf{y}_n + \frac{h}{2} \left( 3\mathbf{f}(t_n, \mathbf{y}_n) - \mathbf{f}(t_{n-1}, \mathbf{y}_{n-1}) \right)$

Al igual que en el caso de una ecuación diferencial única, este método utiliza valores anteriores para predecir el siguiente valor del sistema.

#### **Método de Adams-Moulton para sistemas**
El método **Adams-Moulton**, un método implícito, también se puede aplicar a sistemas de ecuaciones diferenciales. En este caso, se resuelve el sistema de ecuaciones simultáneamente en cada paso.

**Fórmula:**
$\mathbf{y}_{n+1} = \mathbf{y}_n + \frac{h}{2} \left( \mathbf{f}(t_{n+1}, \mathbf{y}_{n+1}) + \mathbf{f}(t_n, \mathbf{y}_n) \right)$

### **Problemas de valor en la frontera (Boundary Value Problems, BVP)**

En un **problema de valor en la frontera**, se busca una solución a una ecuación diferencial que satisfaga condiciones dadas en dos o más puntos (a menudo los extremos de un intervalo).

#### **Método de disparo (Shooting Method)**
El **método de disparo** convierte un problema de valor en la frontera en un problema de valor inicial. La idea es suponer un valor inicial de la derivada en uno de los extremos y resolver la ecuación diferencial como un problema de valor inicial. Luego, se ajusta la derivada inicial hasta que la solución cumpla con la condición en el otro extremo.

**Pasos:**
1. Resolver el problema como un problema de valor inicial.
2. Ajustar iterativamente el valor inicial hasta que la solución satisfaga la condición en el otro extremo.

#### **Método de diferencias finitas**
El **método de diferencias finitas** discretiza la ecuación diferencial en un conjunto de puntos distribuidos a lo largo del intervalo y aproxima las derivadas mediante diferencias finitas.

Para una ecuación de segundo orden, como:
$y'' = f(x, y, y')$
podemos aproximar las derivadas usando fórmulas de diferencias finitas centradas:
$y''(x) \approx \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2}$
Donde $h$ es el tamaño del paso, y $y_i$ es el valor aproximado de la solución en el punto $x_i$.

Este método convierte la ecuación diferencial en un sistema de ecuaciones algebraicas que se puede resolver con métodos numéricos.

#### **Método de collocación**
El **método de collocación** aproxima la solución de la ecuación diferencial utilizando funciones base (normalmente polinomios). Se busca una solución en la forma:
$y(x) = \sum_{i=1}^n c_i \phi_i(x)$

donde $\phi_i(x)$ son las funciones base y $c_i$ son coeficientes a determinar. El problema se convierte en encontrar los valores de $c_i$ que hagan que la solución satisfaga la ecuación en un conjunto de puntos específicos (puntos de collocación).

Este método es útil para obtener soluciones aproximadas con alta precisión en problemas de valor en la frontera.

#### **Método de elementos finitos**
El **método de elementos finitos** (FEM) divide el dominio del problema en pequeños subdominios llamados **elementos**. La solución se aproxima mediante funciones base definidas localmente en cada elemento.

**Pasos:**
1. Dividir el dominio en elementos.
2. Aproximar la solución en cada elemento utilizando funciones polinomiales locales.
3. Formar un sistema de ecuaciones a partir de las ecuaciones diferenciales y las condiciones en la frontera.
4. Resolver el sistema de ecuaciones para obtener la solución aproximada.

Este método es ampliamente utilizado en ingeniería y física para resolver problemas complejos en geometrías irregulares.

#### **Método de diferencias espectrales**
El **método de diferencias espectrales** utiliza funciones globales (como polinomios de Chebyshev o Fourier) para aproximar la solución de un problema de valor en la frontera. Se utiliza una serie de funciones base para aproximar la solución en todo el dominio y luego se ajustan los coeficientes de la serie para satisfacer las

 ecuaciones diferenciales y las condiciones de frontera.

Este método es muy preciso y se utiliza principalmente en problemas donde la solución es suave en todo el dominio.