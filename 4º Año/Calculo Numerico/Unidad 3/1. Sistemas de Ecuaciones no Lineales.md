
### Conceptos Básicos:
El objetivo es encontrar valores de $x$ tales que $f(x)=0$ para una función arbitraria $f(x)$. Las ecuaciones pueden ser algebraicas o trascendentales, dependiendo de la naturaleza de la función $f(x)$.
Existen diversos métodos para hallar estas raices o ceros, dentro de esos métodos encontramos:    

- **Métodos Gráficos**: Se utilizan para obtener una primera aproximación visual de las raíces. Involucran graficar la función y observar los puntos donde cruza el eje $x$.
- **Métodos Cerrados o de Intervalo**: Utilizan el cambio de signo de la función en un intervalo para asegurar la presencia de una raíz. El Método de la Bisección.
- **Métodos Abiertos**: No requieren que la raíz esté encerrada entre dos puntos. Ejemplos incluyen el Método de Newton-Raphson y el Método de la Secante, que pueden converger más rápidamente, pero a veces son susceptibles a la divergencia.

### Métodos de Partición de Intervalos

1. **Método de la Bisección**: es uno de los más sencillos y se utiliza cuando se conoce que la función cambia de signo entre dos valores, es decir, $f(a)$ y $f(b)$ tienen signos opuestos. Este método utiliza el teorema del valor medio y el teorema de Bolzano para encontrar las raíces de una ecuación. El método consta de los siguientes pasos:
	1. **Inicialización**: Selecciona dos puntos $a$ y $b$ tal que $f(a)⋅f(b)<0$.
	2. **Partición**: Se calcula el punto medio $m= \frac{a+b}{2}$.
	3. **Evaluación**: Si $f(m)=0$, entonces $m$ es la raíz. Si no, decide si la raíz está en $[a,m]$ o $[m,b]$ basándote en el signo de $f(m)$.
	4. **Iteración**: Repite el proceso utilizando el nuevo intervalo que contiene la raíz.

>[!note] Consideraciones a tener en Cuenta
>Este método, al igual que el siguiente, solo pueden encontrar una raíz dado un intervalo, en caso de que en dicho intervalo exista más de una sola raíz se deberá partir dicho intervalo en intervalos más pequeño y encontrar cada una de estas raíces aplicando dicho método de forma independiente.

| Ventajas         | Desventajas                                                                                                               |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------- |
| Siempre converge | La convergencia es muy lenta.                                                                                             |
|                  | Si existen más de una raíz en el intervalo, el método no las podrá calcularlo porque solo te permite hallar una sola raíz |

2. **Método de Regula Falsi o Falsa Posición**: Es similar al de la bisección, pero en lugar de tomar el punto medio, usa una interpolación lineal para estimar la raíz. Este método consta de los siguientes pasos:
	1. **Inicialización**: Igual que en la bisección, selecciona $a$ y $b$ donde $f(a)⋅f(b)<0$.
	2. **Interpolación Lineal**: Calcula $x= b - f(b) \times \frac{a−b}{f(b)−f(a)}$, que es el punto donde la línea que conecta $(a,f(a))$ y $(b,f(b))$ cruza el eje $x$.
	3. **Evaluación y Selección del Intervalo**: Similar al método de bisección, usa el signo de $f(x)$ para decidir el nuevo intervalo.
	4. **Iteración**: Repite utilizando el intervalo actualizado.

>[!note] Consideraciones a tener en Cuenta
>Este método tiende a converger más rápido que la bisección, pero puede estancarse en ciertas condiciones.


| Ventajas                                          | Desventajas                                                                                                                |
| ------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| Converge más rápido que el método de la bisección | Si existen más de una raíz en el intervalo, el método no las podrá calcularlo porque solo te permite hallar una sola raíz. |
| Siempre converge                                  |                                                                                                                            |

### Métodos de Iteración de Punto Fijo

1. **Método de Newton-Raphson**: Se basa en la idea de que una curva se puede aproximar localmente como una línea recta y que la intersección de esta línea recta (tangente) con el eje $x$ proporciona una mejor aproximación de la raíz que el punto de partida. Los pasos que sigue este método son los siguientes:
	1. **Inicio**: Selecciona un valor inicial $x_0$ que esté cerca de la raíz esperada.
	2. **Iteración**: Usa la siguiente fórmula para encontrar una mejor aproximación de la raíz: $x_{n+1}= x_n − \frac{f(x_n)}{f'(x_n)}$ donde $f′(x_n)$ es la derivada de $f(x)$ en $x_n$​.
	3. **Repetición**: Repite el paso 2 utilizando el nuevo valor de $x$ hasta que la diferencia entre iteraciones sucesivas sea menor que un umbral de tolerancia o hasta que se alcance un número máximo de iteraciones.

| Ventajas                                                                   | Desventajas                                                                  |
| -------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| Converge más rápido que cualquiera de los métodos analizados anteriormente | La convergencia de este método va a depender de la naturaleza de la función. |
|                                                                            | No es un método conveniente si se tienen raíces múltiples.                   |
|                                                                            | Puede alejarse del área de interes si $f'(x_i)$ tiende a cero.               |

>[!note] Formas de Calcular las Raíces a Través del Método de Newton-Raphson.
>Esté método tiene la particularidad de que puede ser calculado de 3 formas diferentes:
>1. **Método Gráfico**: Se puede obtener la fórmula de Newton-Raphson a partir de un enfoque gráfico donde dibujas la tangente a la curva de la función en el punto inicial y encuentras la intersección de esta tangente con el eje $x$.
>2. **Método de Newton de Segundo Orden**: Si en lugar de considerar los dos primeros términos de la serie de Taylor se consideran los tres primeros términos la aproximación utilizando la siguiente función para ser calculada:
>$$
>x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i) - \frac{f(x_i)}{2.f'(x_i)}.f''(x_i)}
>$$
>3. **Método de la Secante o Newton Lagrange**: surge como una variación del método de Newton-Raphson, en lugar de tomar la tangente se toma la secante. En este método, la derivada se aproxima por una diferencia dividida, por lo que la formula para aproximar a la raíz tiene la siguiente forma:
>$$
>x_{i+1} \cong \frac{x_{i-1}.f(x_i)- x_{i}.f(x_{i+1})}{f(x_i) - f(x_{i-1})}
>$$

>[!tldr] Estimación de Error del Método Newton-Raphson
>El Método de Newton-Raphson tiene una propiedad importante llamada convergencia cuadrática. Esto significa que bajo ciertas condiciones, el número de dígitos correctos en la aproximación se duplica aproximadamente con cada iteración. Para estimar el error en cada iteración, podemos usar varias técnicas:
>1. **Error Absoluto**: Es el valor absoluto de la diferencia entre la nueva aproximación calculada y la aproximación actual.
>2. **Error Relativo**: Es el error absoluto, dividido por el valor absoluto de la nueva aproximación calculada.

2. **Método de Müller**: Este método se utiliza para encontrar raíces de ecuaciones con raíces múltiples, y consiste en obtener los coeficientes de la parábola que pasa por tres puntos elegidos. Dichos coeficientes son sustituidos en la fórmula cuadrática para obtener el valor donde la parábola intersecta al eje $x$; es decir, la raíz estimada. Este método consta de los siguientes pasos:
	1. **Inicialización**: Selecciona tres puntos iniciales, $x_0$, $x_1$​, y $x_2$​.
	2. **Iteración**: Evalúa la función $f$ en los tres puntos y se construye un sistema de ecuaciones lineales de la siguiente forma.
$$
\begin{equation}
\begin{bmatrix}
x_0^2 & x_0 & 1 \\
x_1^2 & x_1 & 1 \\
x_2^2 & x_2 & 1 \\
\end{bmatrix}
\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
=
\begin{bmatrix}
f(x_0) \\
f(x_1) \\
f(x_2) \\
\end{bmatrix}
\end{equation}
$$
Al resultado de esa ecuación se reemplaza los coeficientes de la ecuación cuadrática de la siguiente manera → $ax^2 + bx + c$ y se encuentran las raíces de esa ecuación se evaluan en la función original, si $f(x_a)$ es mayor que $f(x_b)$ se descarta $x_b$ y se reemplaza el punto más antiguo por ese nuevo valor y se vuelve a calcular el sistema nuevamente.

>[!caution] Aclaración
>Las raíces que se encuentran de la solución de la ecuación cuadrática se evaluan en la función original, no en la cuadrática