Las pruebas de desarrollo incluyen todas las actividades de prueba que realiza el equipo que elabora el sistema. Las pruebas de desarrollo tiene como meta descubrir bugs en el software y generalmente están entrelazadas con la depuración. Durante el desarrollo, las pruebas se realizan en tres niveles de granulación:
1. **Pruebas de unidad:** Se pone a prueba unidades de programa o clases de objetos individuales. Las pruebas de unidad deben enfocarse en comprobar la funcionalidad de objetos o métodos. 
2. **Pruebas del componente:** Muchas unidades individuales se integran para crear componentes compuestos. La prueba de componentes debe enfocarse en probar interfaces del componente.
3. **Pruebas del sistema:** Algunos o todos los componentes en un sistema se integran y el sistema se prueba como un todo. Las pruebas del sistema deben enfocarse en poner a prueba las interacciones de los componentes
## Pruebas de Unidad
Las ***pruebas de unidad o unitarias*** son el proceso de probar componentes del programa, como métodos o clases de objetos. Las funciones o los métodos individuales son el tipo más simple de componente. 
Cuando se pone a prueba las clases de objetos, se tiene que diseñar las pruebas para brindar cobertura a todas las características del objeto, esto significa que debe probar todas las operaciones asociadas con el objeto, establecer y verificar el valor de todos los atributos relacionados con el objeto y poner el objeto en todos los estados posibles. Esto quiere decir que tiene que simular todos los eventos que causen un cambio de estado.
La generalización o herencia dificulta la prueba de las clases de objetos, ya que se deben poner a prueba todos los métodos tanto de la clase padre como de la clase hija, independientemente de si estos heredan esos métodos, debido a que el método puede estar sobrecargado o sobreescrito.
### Automatización de Pruebas de Unidad 
Siempre que sea posible, se deben automatizar las pruebas de unidad. En estas pruebas de unidad automatizadas, podría usarse un marco de automatización de pruebas, como JUnit, para escribir y correr las pruebas de programa. Estos marcos de pruebas de unidad ofrecen clases de pruebas genéricas que se extienden para crear casos de prueba específicos. Generalmente, un conjunto automatizado de pruebas consta de tres partes:
- **Configuración**. Se inicializa el sistema con el caso de prueba, esto es, las entradas y salidas esperadas. 
- **Llamada**. Se llama al objeto o al método que se va a probar.
- **Declaración**. Se compara el resultado de la llamada con el resultado esperado. Si la información se evalúa como verdadera, la prueba tuvo éxito; pero si resulta falsa, entonces fracasó.

Las pruebas son costosas y consumen tiempo, así que es importante elegir casos efectivos de pruebas de unidad. La efectividad en los casos de prueba significa que:
1. Los casos de prueba tienen que mostrar que, cuando se usan como se esperaba, el componente que se somete a prueba hace lo que se supone que debe hacer.
2. Si hay defectos en el componente, estos deberían revelarse mediante los casos de prueba.

En consecuencia, hay que escribir dos tipos de casos de prueba. El primero debe reflejar una operación normal de un programa y mostrar que el componente funciona. Y el segundo caso de prueba tiene que basarse en probar la experiencia de donde surgen problemas comunes, se puede hacer uso de entradas anormales para comprobar que se procesan de manera adecuada sin colapsar el componente.

>[!tldr] Lineamientos para Casos de Pruebas de Unidad
>Ejemplos de lineamientos que se pueden utilizar en el diseño de casos de prueba.
>
>1. Elegir entradas que fuercen al sistema a generar todos los mensajes de error. 
>2. Diseñar entradas que produzcan que los buffers de entrada se desborden.
>3. Repetir varias veces la misma entrada o serie de entradas.
>4. Forzar la generación de salidas inválidas. 
>5. Forzar resultados de cálculo demasiado largos o demasiado pequeños.
## Pruebas de Componentes
Los componentes de software son componentes compuestos constituidos por varios objetos en interacción.

![[pruebas_componentes.png]]

Existen diferentes tipos de interfaz entre componentes de programa:
- **Interfaces de parámetro**: Son interfaces en que los datos, o en ocasiones referencias de función, pasan de un componente a otro. Los métodos en un objeto tienen una interfaz de parámetro. 
- **Interfaces de memoria compartida**: Son interfaces en que un bloque de memoria se reparte entre componentes. Los datos se colocan en la memoria de un subsistema y otros subsistemas los recuperan de ahí. Este tipo de interfaz se usa con frecuencia en sistemas embebidos, donde los sensores crean datos que se recuperan y son procesados por otros componentes del sistema. 
- **Interfaces de procedimiento**: Son interfaces en que un componente encapsula un conjunto de procedimientos que pueden ser llamados por otros componentes. Los objetos y otros componentes reutilizables tienen esta forma de interfaz.
- **Interfaces que pasan mensajes**: Se trata de interfaces donde, al enviar un mensaje, un componente solicita un servicio de otro componente. El mensaje de retorno incluye los resultados para ejecutar el servicio.

Existen distintos tipos de error de interfaz que llegan a ocurrir:
- **Uso incorrecto de interfaz**: Un componente que llama a otro componente y comete algún error en el uso de su interfaz. Este tipo de error es común con interfaces de parámetro, donde los parámetros pueden ser del tipo equivocado, o bien, pasar en el orden o el número equivocados de parámetros.
- **Mala interpretación de interfaz**: Un componente que malinterpreta la especificación de la interfaz del componente llamado y hace suposiciones sobre su comportamiento. Por ejemplo, un método de búsqueda binaria puede llamarse con un parámetro que es un arreglo desordenado. Entonces fallaría la búsqueda.
- **Errores de temporización**: Ocurren en sistemas de tiempo real que usan una memoria compartida o una interfaz que pasa mensajes. El productor de datos y el consumidor de datos operan a diferentes niveles de rapidez. A menos que se tenga cuidado particular en el diseño de interfaz, el consumidor puede acceder a información obsoleta, porque el productor de la información no actualizó la información de la interfaz compartida.

>[!tldr] Lineamientos Generales para las Pruebas de Interfaz
>- Examinar el código que se va a probar y listar explícitamente cada llamado a un componente externo. Diseñar un conjunto de pruebas donde los valores de los parámetros hacia los componentes externos estén en los extremos finales de sus rangos. Dichos valores extremos tienen más probabilidad de revelar inconsistencias de interfaz.
>- Donde los punteros pasen a través de una interfaz, probar siempre la interfaz con parámetros de puntero nulo.
>- Donde un componente se llame a través de una interfaz de procedimiento, diseñar pruebas que deliberadamente hagan que falle el componente.
>- Use pruebas de esfuerzo en los sistemas que pasan mensajes. Esto significa que debe diseñar pruebas que generen muchos más mensajes de los que probablemente ocurran en la práctica. Ésta es una forma efectiva de revelar problemas de temporización.
>- Donde algunos componentes interactúen a través de memoria compartida, diseñar pruebas que varíen el orden en que se activan estos componentes. Tales pruebas pueden revelar suposiciones implícitas hechas por el programador, sobre el orden en que se producen y consumen los datos compartidos.
## Pruebas del Sistema
Las pruebas de sistema demuestran que los componentes son compatibles, que interactúan correctamente y que transfieren los datos correctos en el momento adecuado a través de sus interfaces. A menudo se solapan con las pruebas de componentes, pero existen dos importantes diferencias:
1. Durante las pruebas de sistema, los componentes reutilizables desarrollados por separado y los sistemas comerciales pueden integrarse con componentes desarrollados recientemente. Entonces se prueba el sistema completo.
2. Los componentes desarrollados por diferentes miembros del equipo o de grupos pueden integrarse en esta etapa, también se prueban componentes o sistemas reutilizables para acreditar que al integrarse nuevos componentes funcionen como se esperaba. La prueba de sistema es un proceso colectivo más que individual.

En algunas compañías, las pruebas del sistema implican un equipo de prueba independiente, sin la inclusión de diseñadores ni de programadores. 
### Desarrollo Dirigido por Pruebas (TDD)
El desarrollo dirigido por pruebas es una metodología de desarrollo en la que el código se desarrolla incrementalmente, junto con una prueba para ese incremento. No se avanza hacia el siguiente incremento sino hasta que el código diseñado pasa la prueba.

![[ciclo-tdd.png]]

Además de la mejor la comprensión del problema, otros beneficios del desarrollo dirigido por pruebas son:
- **Cobertura de código**: Cualquier segmento de código que escriba debe tener al menos una prueba asociada. El código se prueba a medida que se escribe, de modo que los defectos se descubren con oportunidad en el proceso de desarrollo.
- **Pruebas de regresión**: Un conjunto de pruebas se desarrolla incrementalmente conforme se desarrolla un programa.
- **Depuración simplificada**: Cuando falla una prueba, debe ser evidente dónde yace el problema, evitando el uso de herramientas de depuración para localizar el problema. Es preciso comprobar y modificar el código recién escrito.
- **Documentación del sistema**: Las pruebas en sí actúan como una forma de documentación que describen lo que debe hacer el código. Leer las pruebas suele facilitar la comprensión del código.

Uno de los beneficios más importantes del desarrollo dirigido por pruebas es que reduce los costos de las pruebas de regresión. El desarrollo dirigido por pruebas se usa más en proyectos pequeños y medianos donde de diseña software nuevo, donde la funcionalidad se implementa en código nuevo o usa librerías estándar perfectamente probadas. Si se reutilizan grandes componentes en código o sistemas heredados, entonces se necesita escribir pruebas para dichos sistemas como un todo.
Al emplear este método, se necesitará de un proceso de *prueba de sistema* para validar el sistema; esto es, comprobar que cumple con los requerimientos de todos los participantes del sistema.
