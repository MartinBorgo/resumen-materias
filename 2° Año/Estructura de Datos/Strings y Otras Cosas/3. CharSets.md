## Representación en una Computadora
Para que se ejecute un programa es necesario dos tipos de información:

1. Las **instrucciones** que forman el programa.
2. Los **datos** con los que debe operar ese programa.

Dos aspectos importantes relacionados con la información son cómo ***representarla*** y cómo ***registrarla*** físicamente. Desde el punto de vista de ***abstracción de datos***, la forma de guardar la información en un medio digital, establece la representación interna de un ***tipo de datos***.
Como no pueden representarse conceptos, ideas directamente en un medio digital, se debe llegar a un elemento de más bajo nivel: ***el carácter.*** A partir de estos caracteres, se pueden estructurar formas mas expresivas, string, palabras, entre otras cosas.
Desde el punto de vista de la ***abstracción de datos***, se debe establecer una forma de representar el ***tipo de dato carácter***, el cual es predefinido y primitivo en la mayoría de los Lenguajes de programación.
Al tener que “traducir” toda la información suministrada a la computadora a 0s y 1s, es necesario establecer una correspondencia entre el conjunto de todos los caracteres, que lo llamaremos **α** y el conjunto binario que lo llamaremos **β**
Es decir, es necesario hacer una codificación o representación de los elementos de un conjunto (en este caso α) mediante los de otro conjunto (β) de forma tal que a cada elemento de α le corresponda un elemento distinto de β (n bits).
Estos método de transformación se denominan tabla de códigos de entrada/salida, sistemas de codificación de caracteres.

> ![[ED-RepresentacionBinaria.png]]

- Una codificación de caracteres es una correspondencia entre los conjuntos α y β, tal como vemos en la imagen la relación es en ambos lados
- Los códigos se pueden definir de forma arbitraria.
- No obstante existen codificaciones normalizadas y estandarizado que son utilizados por diferentes constructores de computadoras.

#### Dependencia del tamaño
Supóngase que se utiliza n bits para codificar los símbolos de α. El valor mínimo de n dependerá de m, número de elementos de α. Es decir, que la cantidad de caracteres que podamos representar esta limitado por la cantidad de bits (m) que se usen para la representación de dicho conjunto de caracterers.

---

Una de las tablas mas conocidas en el ASCII que contaba con 128 caracteres representados, pero este sistema de representación tenia una limitación que era el no poder representar los caracteres ortográficos de otros distintos al ingles.
El primer intento de estender este sistema fue ek IS 646 o el mejor llamado Latin-1 y el segund intento fue el IS 8859 que introdujo en concepto de paginas de código.

- El problema es que el software debe conocer en qué página de código trabaja, no es posible combinar páginas y quedan fuera del esquema lenguajes como el chino y japonés.

#### La creación de UNICODE
Surge para solucionar los problemas del ASCII y sus extensiones. La idea es asignar a cada carácter y símbolo un valor único y permanente, llamado ***punto de código*** esta forma de representar a los caracteres, sin importar la plataforma, sin importar el programa, sin importar el idioma.

- **Principios de diceño:**
	- Universalidad: Un repertorio suficientemente amplio que albergue a todos los caracteres probables en el intercambio de texto multilingüe.
	- Eficiencia: Las secuencias generadas deben ser fáciles de tratar.
	- No Ambigüedad: Un código dado siempre representa el mismo carácter.

- **Etapas del modelo de caracteres:**
	1. *Conjunto abstracto de caracteres:* Un caracter es el componente más pequeño de un lenguaje escrito que tiene valor semántico, y cada idioma tiene su propio sistema de escritura y de organización.
	2. *Conjunto de caracteres codificados:* Este conjunto, da un nombre y un punto de código a cada carácter abstracto, cuando se le asigna un código a un carácter, se dice que dicho carácter está codificado. El conjunto de caracteres codificados por Unicode, es la Base de Datos Unicode o UCD, dicha base de datos esta dividida en planos y estos a su vez en ***áreas*** y ***bloques***.
	3. *Formas de codificación de caracteres UTFs:* Las formas de codificación de Unicode reglamentan la forma en que los puntos de código se transformarán en unidades tratables por el medio digital. Unicode define tres formas de codificación o UTFs:
		- UTF-8 -> usa 8 bits por unidad de código. Cada carácter es representado con 1 a 4 unidades de código. Por eso se dice que son códigos de longitud variable.
		- UTF-16 -> usa unidad de código de 16 bits. Cada carácter es representado con 1 ó 2 unidades. Optimizada para la representación del plano 0 básico multilingüe.
		- UTF-32 -> usa 1 unidad de codigo de 32 bits. Son representaciones de longitud fija.
	4. *Esquemas de codificación de caracteres:* La capa final del modelo de caracteres se ocupa de la serialización en bytes de las unidades de código. de como seran representaros en los distintos UTFs y la forma en la que los bits van a ser ubicados y leidos, de los cuales existen 2 metodos:

>***Big-endian:*** los primeros bits son los leidos y puestos en primer ligar.
> ![[ED-bigEndian.png]]
***Little-endian:*** los primeros bits son los primeros leidos pero se ubican al final.
>![[ED-littleEndian.png]]

